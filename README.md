# BRIDGE Dataset

**BRIDGE** is a RAG benchmark that enables comprehensive evaluation of both **retrieval** and **generation**, while accurately revealing their **alignment**.

It consists of two core components:

1. **Answer Validation Data**
    - Includes: `query`, `gold_answer`, `generated_answer`, and `answer_validation` (human judgment)
2. **RAG Dataset**
    - Includes: `query`, `gold_chunk`, and `gold_answer` (with corrected retrieval labels)

---

## ðŸ“¦ Dataset Fields

| Field             | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| `q_id`           | Unique identifier for each query                                            |
| `query`          | Input question from a QA dataset                                            |
| `gold_answer`    | Ground-truth answer for the given query                                     |
| `generated_answer` | Answer generated by an LLM using Top-10 retrieved chunks from 16 retrieval methods  |
| `answer_validation` | Human label for the generated answer (1 = correct, 0 = incorrect)       |
| `gold_chunk`     | Correctly labeled chunk for the query, obtained via our chunk label correction pipeline |


